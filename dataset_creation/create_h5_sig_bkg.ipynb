{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4152a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for creating a training and testing samples from v2_sig_bkg/*.parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6d6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import awkward as ak\n",
    "import h5py\n",
    "import vector\n",
    "vector.register_numba()\n",
    "vector.register_awkward()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import mplhep as hep\n",
    "hep.style.use(hep.style.ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf999de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(file):\n",
    "    file.create_group(\"TARGETS/t1\") # hadronic top -> q1 q2 b\n",
    "    file.create_group(\"TARGETS/t2\") # leptonic top -> b\n",
    "    file.create_group(\"TARGETS/h\") # higgs -> b1 b2\n",
    "    file.create_group(\"INPUTS\")\n",
    "    file.create_group(\"INPUTS/Source\")\n",
    "    return file\n",
    "\n",
    "def create_targets(file, particle, jets):\n",
    "    multiindex = ak.zip([ak.local_index(jets, i) for i in range(jets.ndim)])\n",
    "    \n",
    "    if particle == \"h\":\n",
    "        mask = jets.prov == 1 # H->b1b2\n",
    "        multiindex2 = multiindex[mask]\n",
    "        \n",
    "        b1_array = []\n",
    "        b2_array = []\n",
    "\n",
    "        for index,i in enumerate(multiindex2):\n",
    "            if len(i) == 0:\n",
    "                b1_array.append(-1)\n",
    "                b2_array.append(-1)\n",
    "            elif len(i) == 1:\n",
    "                b1_array.append(i[0].tolist()[1])\n",
    "                b2_array.append(-1)\n",
    "            elif len(i) == 2:\n",
    "                b1_array.append(i[0].tolist()[1])\n",
    "                b2_array.append(i[1].tolist()[1])\n",
    "        \n",
    "        file.create_dataset(\"TARGETS/h/b1\", np.shape(b1_array), dtype='int64', data=b1_array)\n",
    "        file.create_dataset(\"TARGETS/h/b2\", np.shape(b2_array), dtype='int64', data=b2_array)\n",
    "        \n",
    "    elif particle == \"t1\":\n",
    "        mask = jets.prov == 5 # W->q1q2 from t1\n",
    "        multiindex2 = multiindex[mask]\n",
    "        \n",
    "        q1_array = []\n",
    "        q2_array = []\n",
    "\n",
    "        for index,i in enumerate(multiindex2):\n",
    "            if len(i) == 0:\n",
    "                q1_array.append(-1)\n",
    "                q2_array.append(-1)\n",
    "            elif len(i) == 1:\n",
    "                q1_array.append(i[0].tolist()[1])\n",
    "                q2_array.append(-1)\n",
    "            elif len(i) == 2:\n",
    "                q1_array.append(i[0].tolist()[1])\n",
    "                q2_array.append(i[1].tolist()[1])\n",
    "                \n",
    "        mask = jets.prov == 2 # t1->Wb \n",
    "        multiindex2 = multiindex[mask]\n",
    "        \n",
    "        had_b_array = []\n",
    "\n",
    "        for index,i in enumerate(multiindex2):\n",
    "            if len(i) == 0:\n",
    "                had_b_array.append(-1)\n",
    "            elif len(i) == 1:\n",
    "                had_b_array.append(i[0].tolist()[1])\n",
    "                \n",
    "        file.create_dataset(\"TARGETS/t1/q1\", np.shape(q1_array), dtype='int64', data=q1_array)\n",
    "        file.create_dataset(\"TARGETS/t1/q2\", np.shape(q2_array), dtype='int64', data=q2_array)\n",
    "        file.create_dataset(\"TARGETS/t1/b\", np.shape(had_b_array), dtype='int64', data=had_b_array)\n",
    "                \n",
    "    elif particle == \"t2\":\n",
    "        mask = jets.prov == 3 # t2->b \n",
    "        multiindex2 = multiindex[mask]\n",
    "        \n",
    "        lep_b_array = []\n",
    "\n",
    "        for index,i in enumerate(multiindex2):\n",
    "            if len(i) == 0:\n",
    "                lep_b_array.append(-1)\n",
    "            elif len(i) == 1:\n",
    "                lep_b_array.append(i[0].tolist()[1])\n",
    "\n",
    "        file.create_dataset(\"TARGETS/t2/b\", np.shape(lep_b_array), dtype='int64', data=lep_b_array)\n",
    "\n",
    "def create_inputs(file, jets):\n",
    "    pt_array = ak.to_numpy(ak.fill_none(ak.pad_none(jets.pt, ak.max(ak.num(jets)), clip=True), 0))\n",
    "    mask = ~(pt_array == 0)\n",
    "    mask_ds = file.create_dataset(\"INPUTS/Source/MASK\", np.shape(mask), dtype='bool', data=mask)\n",
    "    pt_ds = file.create_dataset(\"INPUTS/Source/pt\", np.shape(pt_array), dtype='float32', data=pt_array)\n",
    "\n",
    "    phi_array = ak.to_numpy(ak.fill_none(ak.pad_none(jets.phi, ak.max(ak.num(jets)), clip=True), 0))\n",
    "    phi_ds = file.create_dataset(\"INPUTS/Source/phi\", np.shape(phi_array), dtype='float32', data=phi_array)\n",
    "\n",
    "    eta_array = ak.to_numpy(ak.fill_none(ak.pad_none(jets.eta, ak.max(ak.num(jets)), clip=True), 0))\n",
    "    eta_ds = file.create_dataset(\"INPUTS/Source/eta\", np.shape(eta_array), dtype='float32', data=eta_array)\n",
    "\n",
    "    btag = ak.to_numpy(ak.fill_none(ak.pad_none(jets.btag, ak.max(ak.num(jets)), clip=True), 0))\n",
    "    btag_ds = file.create_dataset(\"INPUTS/Source/btag\", np.shape(btag), dtype='float32', data=btag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492cc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"/eos/user/y/ymaidann/eth_project/Spanet_project/v2_sig_bkg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39416f29",
   "metadata": {},
   "source": [
    "## Signal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "407fe702",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"ttHTobb_2016_PostVFP\", \n",
    "         \"ttHTobb_2016_PreVFP\",\n",
    "         \"ttHTobb_2017\",\n",
    "         \"ttHTobb_2018\"]\n",
    "files = []\n",
    "for name in names:\n",
    "    files.append(\"all_jets_fullRun2_\"+name+\"_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0fece1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    df = ak.from_parquet(basedir + files[i])\n",
    "    (jets,_,_,_,_,_,_,_,_) = ak.unzip(df)\n",
    "    \n",
    "    # Get fully matched jets from df\n",
    "    mask_fullymatched = ak.sum(jets.matched == True, axis=1)>=6\n",
    "    higgs = jets[jets.prov == 1]\n",
    "    jets = jets[ak.num(higgs) == 2]\n",
    "\n",
    "    w_or_t_jets = jets[(jets.prov == 5)|(jets.prov == 2)]\n",
    "    jets = jets[ak.num(w_or_t_jets) == 3]\n",
    "\n",
    "    lep_top = jets[jets.prov == 3]\n",
    "    jets = jets[ak.num(lep_top) == 1]\n",
    "    \n",
    "    output_file = h5py.File(names[i]+\"_matched.h5\", \"w\")\n",
    "    output_file = create_groups(output_file)\n",
    "    \n",
    "    # Create target arrays in the files. This will take a few minutes\n",
    "    create_targets(output_file, \"h\", jets)\n",
    "    create_targets(output_file, \"t1\", jets)\n",
    "    create_targets(output_file, \"t2\", jets)\n",
    "    \n",
    "    # Create input arrays in the files\n",
    "    create_inputs(output_file, jets)\n",
    "    output_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda0f83",
   "metadata": {},
   "source": [
    "## Backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a986a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"TTbbSemiLeptonic_Powheg_2016_PostVFP\", \n",
    "         \"TTbbSemiLeptonic_Powheg_2016_PreVFP\",\n",
    "         \"TTbbSemiLeptonic_Powheg_2017\",\n",
    "         \"TTbbSemiLeptonic_Powheg_2018\",\n",
    "         \"TTToSemiLeptonic_2016_PostVFP\",\n",
    "         \"TTToSemiLeptonic_2016_PreVFP\",\n",
    "         \"TTToSemiLeptonic_2017\",\n",
    "         \"TTToSemiLeptonic_2018\"]\n",
    "names = [\"TTbbSemiLeptonic_Powheg_2016_PostVFP\"]\n",
    "files = []\n",
    "for name in names:\n",
    "    files.append(\"all_jets_fullRun2_\"+name+\"_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed30ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets_bkg(file, particle, jets):\n",
    "   \n",
    "    if particle == \"h\":\n",
    "\n",
    "        b1_array = np.ones(len(jets)) \n",
    "        b2_array = np.ones(len(jets))\n",
    "        b1_array *= -1\n",
    "        b2_array *= -1\n",
    "        \n",
    "        file.create_dataset(\"TARGETS/h/b1\", np.shape(b1_array), dtype='int64', data=b1_array)\n",
    "        file.create_dataset(\"TARGETS/h/b2\", np.shape(b2_array), dtype='int64', data=b2_array)\n",
    "        \n",
    "    else:\n",
    "        create_targets(file, particle, jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    df = ak.from_parquet(basedir + files[i])\n",
    "    (jets,_,_,_,_,_,_,_) = ak.unzip(df)\n",
    "    \n",
    "    jets = jets[1000000:]\n",
    "    output_file = h5py.File(names[i]+\"2.h5\", \"w\")\n",
    "    output_file = create_groups(output_file)\n",
    "\n",
    "    # Create target arrays in the files. This will take a few minutes\n",
    "    create_targets_bkg(output_file, \"h\", jets)\n",
    "    create_targets_bkg(output_file, \"t1\", jets)\n",
    "    create_targets_bkg(output_file, \"t2\", jets)\n",
    "    \n",
    "    # Create input arrays in the files\n",
    "    create_inputs(output_file, jets)\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f889416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
